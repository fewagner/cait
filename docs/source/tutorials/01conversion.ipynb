{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conversion of Hardware-triggered Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we generate mock hardware triggered data to test all functionality of Cait. The generated data is in all properties similar to data from the CRESST and COSINUS data aquisitions that work with the program CSS. The only exception is that the noise and pulses are not measured, but generated with parametric descriptions of the pulse shape and normal distributed noise.\n",
    "\n",
    "We start with importing the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:04:10.893348Z",
     "start_time": "2021-11-10T11:04:07.126015Z"
    }
   },
   "outputs": [],
   "source": [
    "import cait as ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Generate Test Data\n",
    "(You can skip this step if you already have hardware triggered data from your experiment for example.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TestData class handles the generation of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:04:12.796679Z",
     "start_time": "2021-11-10T11:04:10.897815Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = ai.data.TestData(filepath='test_data/mock_001', duration=1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate an RDT file, that holds all triggered events, test pulses and noise events. Right after the generation we call a check function, that prints the content of the first event, to check if the file is properly written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:04:14.006174Z",
     "start_time": "2021-11-10T11:04:12.799908Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rdt file written.\n",
      "DataHandler Instance created.\n",
      "#############################################################\n",
      "EVENT NUMBER:  0\n",
      "detector number (starting at 0):  0\n",
      "number of coincident pulses in digitizer module:  0\n",
      "module trigger counter (starts at 0, when TRA or WRITE starts):  1\n",
      "channel trigger delay relative to time stamp [Âµs]:  0\n",
      "absolute time [s] (computer time timeval.tv_sec):  1602879726\n",
      "absolute time [us] (computer time timeval.tv_us):  0\n",
      "Delay of channel trigger to testpulse [us]:  0\n",
      "time stamp of module trigger low word (10 MHz clock, 0 @ START WRITE ):  0\n",
      "time stamp of module trigger high word (10 MHz clock, 0 @ START WRITE ):  6\n",
      "number of qdc events accumulated until digitizer trigger:  0\n",
      "measuring hours (0 @ START WRITE):  0.0016666667070239782\n",
      "accumulated dead time of channel [s] (0 @ START WRITE):  0.0\n",
      "test pulse amplitude (0. for pulses, (0.,10.] for test pulses, >10. for control pulses):  0.10000000149011612\n",
      "DAC output of control program (proportional to heater power):  0.0\n"
     ]
    }
   ],
   "source": [
    "test_data._generate_rdt_file()\n",
    "dh = ai.DataHandler(nmbr_channels=2)\n",
    "dh.checkout_rdt(path_rdt='test_data/mock_001.rdt', read_events=1, verb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CON file holds the time stamps and pulse heights of the control pulses. Also for the CON file, we call a check function after the data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:04:14.047118Z",
     "start_time": "2021-11-10T11:04:14.008714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con file written.\n",
      "DataHandler Instance created.\n",
      "5 control pulses read from CON file.\n",
      " \tdetector_nmbr,\t \tpulse_height, \ttime_stamp_low, \ttime_stamp_high, \tdead_time, \tmus_since_last_tp\n",
      "1\t0\t\t6.27\t\t30000000\t\t0\t\t\t0.0\t[0]\n",
      "2\t1\t\t4.06\t\t30000000\t\t0\t\t\t0.0\t[0]\n",
      "3\t0\t\t6.21\t\t120000000\t\t0\t\t\t0.0\t[0]\n",
      "4\t1\t\t3.96\t\t120000000\t\t0\t\t\t0.0\t[0]\n",
      "5\t0\t\t5.71\t\t210000000\t\t0\t\t\t0.0\t[0]\n"
     ]
    }
   ],
   "source": [
    "test_data._generate_con_file()\n",
    "dh = ai.DataHandler(nmbr_channels=2)\n",
    "dh.checkout_con(path_con='test_data/mock_001.con', read_events=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To every RDT file belongs a PAR file, which is a text file with additional information. The generated PAR file can be checked by opening it with a text editor or with \"vim FILE_NAME\" in the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:04:14.055098Z",
     "start_time": "2021-11-10T11:04:14.049992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par file written.\n"
     ]
    }
   ],
   "source": [
    "test_data._generate_par_file()\n",
    "# test by looking at the text file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the data generation for a second file, this time we call a pre-implemented method that does the steps from above all at once. Notice that we specify the gap in measuring time between the two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:04:15.038081Z",
     "start_time": "2021-11-10T11:04:14.057678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rdt file written.\n",
      "Con file written.\n",
      "Par file written.\n"
     ]
    }
   ],
   "source": [
    "test_data.update_filepath(file_path='test_data/mock_002')\n",
    "test_data.generate(start_offset=1.5 * 3600, source='hw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cait library accesses and stores its data in HDF5 files, which are a structured file format and convenient for high-level applications. We are aware, that saving the data twice might be inefficient in terms of storage space. However, as a solution for this we propose to keep the raw data events only so long in the HDF5 files, until all needed high level features of the raw data are calculated. We show below how this is done.\n",
    "\n",
    "But first, we generate a HDF5 file from the events that are contained in the RDT file and the control pulses from the CON file. For this, the PAR file must be in the same directory as the RDT file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:04:15.042599Z",
     "start_time": "2021-11-10T11:04:15.040128Z"
    }
   },
   "outputs": [],
   "source": [
    "path_data = 'test_data/'\n",
    "file_names = ['mock_001',\n",
    "              'mock_002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:04:16.108374Z",
     "start_time": "2021-11-10T11:04:15.047616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler Instance created.\n",
      "Start converting.\n",
      "\n",
      "READ EVENTS FROM RDT FILE.\n",
      "Total Records in File:  800\n",
      "Getting good idx. (Depending on OS and drive reading speed, this might take some minutes!)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d952655186f8455a8384139ad72516d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Counts Channel 0: 400\n",
      "Event Counts Channel 1: 400\n",
      "Getting good tpas.\n",
      "Good consecutive counts: 400\n",
      "\n",
      "WORKING ON EVENTS WITH TPA = 0.\n",
      "CREATE DATASET WITH EVENTS.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c33bf5426b4b0589c0142f95092ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WORKING ON EVENTS WITH TPA = -1.\n",
      "CREATE DATASET WITH NOISE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec232e57128548eea3bf27cfb076e92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WORKING ON EVENTS WITH TPA > 0.\n",
      "CREATE DATASET WITH TESTPULSES.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fb728421e942b78d616b5270b72206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hdf5 dataset created in  test_data/\n",
      "Filepath and -name saved.\n",
      "Accessing CON File...\n",
      "200 Control Pulses for channel 0 in file.\n",
      "CON File included.\n",
      "DataHandler Instance created.\n",
      "Start converting.\n",
      "\n",
      "READ EVENTS FROM RDT FILE.\n",
      "Total Records in File:  800\n",
      "Getting good idx. (Depending on OS and drive reading speed, this might take some minutes!)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24fea76ff8849c2856decca5364c26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Counts Channel 0: 400\n",
      "Event Counts Channel 1: 400\n",
      "Getting good tpas.\n",
      "Good consecutive counts: 400\n",
      "\n",
      "WORKING ON EVENTS WITH TPA = 0.\n",
      "CREATE DATASET WITH EVENTS.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e533e555a645f0a0e7c8dd02309af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WORKING ON EVENTS WITH TPA = -1.\n",
      "CREATE DATASET WITH NOISE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfd7b46af8742e2abb0b96e5ce3afc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WORKING ON EVENTS WITH TPA > 0.\n",
      "CREATE DATASET WITH TESTPULSES.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1728270fba354c1393aab07afffcb33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hdf5 dataset created in  test_data/\n",
      "Filepath and -name saved.\n",
      "Accessing CON File...\n",
      "200 Control Pulses for channel 0 in file.\n",
      "CON File included.\n"
     ]
    }
   ],
   "source": [
    "# Conversion from Rdt to HDF5\n",
    "for file in file_names:\n",
    "    dh = ai.DataHandler(channels=[0,1],\n",
    "                        record_length=16384,\n",
    "                        sample_frequency=25000)\n",
    "    \n",
    "    dh.convert_dataset(\n",
    "        path_rdt=path_data,\n",
    "        fname=file,\n",
    "        path_h5=path_data,\n",
    "        tpa_list=[0, 1, -1],\n",
    "        calc_mp=False,\n",
    "        calc_sev=False,\n",
    "        calc_nps=False,\n",
    "        lazy_loading=True,\n",
    "        event_dtype='float32',\n",
    "        ints_in_header=7,\n",
    "        memsafe=True,\n",
    "        dvm_channels=0,\n",
    "        batch_size=1000,\n",
    "        trace=False,\n",
    "    )\n",
    "\n",
    "    dh.include_con_file(path_con_file=path_data + file + '.con')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we called for the first time the `DataHandler` class. This is a heavy class, that handles all the feature calculations of the raw data. It has stored the path to the HDF5 file as an attribute and saves all calculated properties there. You can get an overview of what data is stored in the `DataHandler` by calling its `content` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95mcontrolpulses\u001b[0m\n",
      "  \u001b[1m\u001b[36mhours                  \u001b[0m\u001b[93m    \u001b[0m (200,)    float64\n",
      "  \u001b[1m\u001b[36mpulse_height           \u001b[0m\u001b[93m    \u001b[0m (2, 200)  float64\n",
      "\u001b[1m\u001b[95mevents\u001b[0m\n",
      "  \u001b[1m\u001b[36mdac_output             \u001b[0m\u001b[93m    \u001b[0m (80,)           float64\n",
      "  \u001b[1m\u001b[36mevent                  \u001b[0m\u001b[93m    \u001b[0m (2, 80, 16384)  float32\n",
      "  \u001b[1m\u001b[36mhours                  \u001b[0m\u001b[93m    \u001b[0m (80,)           float64\n",
      "  \u001b[1m\u001b[36mtime_mus               \u001b[0m\u001b[93m    \u001b[0m (80,)           int32\n",
      "  \u001b[1m\u001b[36mtime_s                 \u001b[0m\u001b[93m    \u001b[0m (80,)           int32\n",
      "\u001b[1m\u001b[95mnoise\u001b[0m\n",
      "  \u001b[1m\u001b[36mdac_output             \u001b[0m\u001b[93m    \u001b[0m (80,)           float64\n",
      "  \u001b[1m\u001b[36mevent                  \u001b[0m\u001b[93m    \u001b[0m (2, 80, 16384)  float32\n",
      "  \u001b[1m\u001b[36mhours                  \u001b[0m\u001b[93m    \u001b[0m (80,)           float64\n",
      "  \u001b[1m\u001b[36mtime_mus               \u001b[0m\u001b[93m    \u001b[0m (80,)           int32\n",
      "  \u001b[1m\u001b[36mtime_s                 \u001b[0m\u001b[93m    \u001b[0m (80,)           int32\n",
      "\u001b[1m\u001b[95mtestpulses\u001b[0m\n",
      "  \u001b[1m\u001b[36mdac_output             \u001b[0m\u001b[93m    \u001b[0m (240,)           float64\n",
      "  \u001b[1m\u001b[36mevent                  \u001b[0m\u001b[93m    \u001b[0m (2, 240, 16384)  float32\n",
      "  \u001b[1m\u001b[36mhours                  \u001b[0m\u001b[93m    \u001b[0m (240,)           float64\n",
      "  \u001b[1m\u001b[36mtestpulseamplitude     \u001b[0m\u001b[93m    \u001b[0m (240,)           float64\n",
      "  \u001b[1m\u001b[36mtime_mus               \u001b[0m\u001b[93m    \u001b[0m (240,)           int32\n",
      "  \u001b[1m\u001b[36mtime_s                 \u001b[0m\u001b[93m    \u001b[0m (240,)           int32\n"
     ]
    }
   ],
   "source": [
    "dh.content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine multiple files\n",
    "We are often in the position, that we want to process data from multiple consecutive measurements in common. For this, we can merge two converted files and specify, if we want to keep the individual files. For large scale data processing, were often events from more than a hundred RDT files are processed, it makes sense to only *virtually link* the files without copying any data. This can be achieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file 'test_data/combined_file-P_Ch0-L_Ch1.h5'.\n",
      "Successfully combined files ['mock_001-P_Ch0-L_Ch1', 'mock_002-P_Ch0-L_Ch1'] into 'test_data/combined_file-P_Ch0-L_Ch1.h5' (18.0 KiB).\n",
      "Calculating extended \u001b[1m\u001b[36mhours\u001b[0m for all groups with datasets \u001b[1m\u001b[36mevent\u001b[0m, \u001b[1m\u001b[36mhours\u001b[0m, \u001b[1m\u001b[36mtime_s\u001b[0m, \u001b[1m\u001b[36mtime_mus\u001b[0m:\n",
      "DataHandler Instance created.\n",
      "Successfully written \u001b[1m\u001b[36mhours\u001b[0m with shape (160,) and dtype 'float32' to group \u001b[1m\u001b[95mevents\u001b[0m.\n",
      "\n",
      "Successfully written \u001b[1m\u001b[36mhours\u001b[0m with shape (160,) and dtype 'float32' to group \u001b[1m\u001b[95mnoise\u001b[0m.\n",
      "\n",
      "Successfully written \u001b[1m\u001b[36mhours\u001b[0m with shape (480,) and dtype 'float32' to group \u001b[1m\u001b[95mtestpulses\u001b[0m.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ai.data.combine_h5(fname=\"combined_file-P_Ch0-L_Ch1\",\n",
    "                   files=[fn+'-P_Ch0-L_Ch1' for fn in file_names],\n",
    "                   src_dir=path_data,\n",
    "                   out_dir=path_data,\n",
    "                   groups_combine=[\"events\", \"testpulses\", \"controlpulses\", \"noise\"]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we keep the original files and only create a third one (with very little disk space usage) that links to the original ones. If you really want to *merge* the files, i.e. copy all the original data into one file, use `ai.data.merge_h5` instead.\n",
    "\n",
    "Now we create a `DataHandler` for the combined file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:04:16.286181Z",
     "start_time": "2021-11-10T11:04:16.280540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler Instance created.\n",
      "\u001b[1m\u001b[95mcontrolpulses\u001b[0m\n",
      "  \u001b[1m\u001b[36mhours                  \u001b[0m\u001b[93m (v)\u001b[0m (400,)    float64\n",
      "  \u001b[1m\u001b[36mpulse_height           \u001b[0m\u001b[93m (v)\u001b[0m (2, 400)  float64\n",
      "\u001b[1m\u001b[95mevents\u001b[0m\n",
      "  \u001b[1m\u001b[36mdac_output             \u001b[0m\u001b[93m (v)\u001b[0m (160,)           float64\n",
      "  \u001b[1m\u001b[36mevent                  \u001b[0m\u001b[93m (v)\u001b[0m (2, 160, 16384)  float32\n",
      "  \u001b[1m\u001b[36mhours                  \u001b[0m\u001b[93m    \u001b[0m (160,)           float32\n",
      "  \u001b[1m\u001b[36mtime_mus               \u001b[0m\u001b[93m (v)\u001b[0m (160,)           int32\n",
      "  \u001b[1m\u001b[36mtime_s                 \u001b[0m\u001b[93m (v)\u001b[0m (160,)           int32\n",
      "\u001b[1m\u001b[95mnoise\u001b[0m\n",
      "  \u001b[1m\u001b[36mdac_output             \u001b[0m\u001b[93m (v)\u001b[0m (160,)           float64\n",
      "  \u001b[1m\u001b[36mevent                  \u001b[0m\u001b[93m (v)\u001b[0m (2, 160, 16384)  float32\n",
      "  \u001b[1m\u001b[36mhours                  \u001b[0m\u001b[93m    \u001b[0m (160,)           float32\n",
      "  \u001b[1m\u001b[36mtime_mus               \u001b[0m\u001b[93m (v)\u001b[0m (160,)           int32\n",
      "  \u001b[1m\u001b[36mtime_s                 \u001b[0m\u001b[93m (v)\u001b[0m (160,)           int32\n",
      "\u001b[1m\u001b[95mtestpulses\u001b[0m\n",
      "  \u001b[1m\u001b[36mdac_output             \u001b[0m\u001b[93m (v)\u001b[0m (480,)           float64\n",
      "  \u001b[1m\u001b[36mevent                  \u001b[0m\u001b[93m (v)\u001b[0m (2, 480, 16384)  float32\n",
      "  \u001b[1m\u001b[36mhours                  \u001b[0m\u001b[93m    \u001b[0m (480,)           float32\n",
      "  \u001b[1m\u001b[36mtestpulseamplitude     \u001b[0m\u001b[93m (v)\u001b[0m (480,)           float64\n",
      "  \u001b[1m\u001b[36mtime_mus               \u001b[0m\u001b[93m (v)\u001b[0m (480,)           int32\n",
      "  \u001b[1m\u001b[36mtime_s                 \u001b[0m\u001b[93m (v)\u001b[0m (480,)           int32\n"
     ]
    }
   ],
   "source": [
    "dh_combined = ai.DataHandler(channels=[0, 1], record_length=16384, sample_frequency=25000)\n",
    "dh_combined.set_filepath(path_h5=path_data, fname=\"combined_file\")\n",
    "dh_combined.content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the datasets have a **(v)** marker now which tells us that we are looking at a **v**irtual dataset, i.e. that it is only a reference to the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete event traces\n",
    "Once you are done with your raw data analysis (usually once you have your final energy spectra), you might wish to delete the raw event voltage traces because they take up a lot of disk space. You can do this (in this example already for all files) like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler Instance created.\n",
      "Dataset \u001b[1m\u001b[36mevent\u001b[0m deleted from group \u001b[1m\u001b[95mevents\u001b[0m.\n",
      "Dataset \u001b[1m\u001b[36mevent\u001b[0m deleted from group \u001b[1m\u001b[95mtestpulses\u001b[0m.\n",
      "Dataset \u001b[1m\u001b[36mevent\u001b[0m deleted from group \u001b[1m\u001b[95mnoise\u001b[0m.\n",
      "Successfully repackaged 'test_data/mock_001-P_Ch0-L_Ch1.h5'. Memory saved: 50.0 MiB\n",
      "DataHandler Instance created.\n",
      "Dataset \u001b[1m\u001b[36mevent\u001b[0m deleted from group \u001b[1m\u001b[95mevents\u001b[0m.\n",
      "Dataset \u001b[1m\u001b[36mevent\u001b[0m deleted from group \u001b[1m\u001b[95mtestpulses\u001b[0m.\n",
      "Dataset \u001b[1m\u001b[36mevent\u001b[0m deleted from group \u001b[1m\u001b[95mnoise\u001b[0m.\n",
      "Successfully repackaged 'test_data/mock_002-P_Ch0-L_Ch1.h5'. Memory saved: 50.0 MiB\n"
     ]
    }
   ],
   "source": [
    "for fn in file_names:\n",
    "    dh = ai.DataHandler(channels=[0, 1], record_length=16384, sample_frequency=25000)\n",
    "    dh.set_filepath(path_h5=path_data, fname=fn)\n",
    "    dh.drop_raw_data(\"events\")\n",
    "    dh.drop_raw_data(\"testpulses\")\n",
    "    dh.drop_raw_data(\"noise\")\n",
    "    # Dropping datasets does NOT decrease the size of an HDF5 file on disk because\n",
    "    # of its file structure. To actually reduce the size, you have to repackage it\n",
    "    dh.repackage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a file that combines all the original files, you would need to run the `cait.data.combine_h5` function again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file 'test_data/combined_file-P_Ch0-L_Ch1.h5'.\n",
      "Successfully combined files ['mock_001-P_Ch0-L_Ch1', 'mock_002-P_Ch0-L_Ch1'] into 'test_data/combined_file-P_Ch0-L_Ch1.h5' (13.2 KiB).\n",
      "Calculating extended \u001b[1m\u001b[36mhours\u001b[0m for all groups with datasets \u001b[1m\u001b[36mevent\u001b[0m, \u001b[1m\u001b[36mhours\u001b[0m, \u001b[1m\u001b[36mtime_s\u001b[0m, \u001b[1m\u001b[36mtime_mus\u001b[0m:\n",
      "DataHandler Instance created.\n",
      "Successfully written \u001b[1m\u001b[36mhours\u001b[0m with shape (160,) and dtype 'float32' to group \u001b[1m\u001b[95mevents\u001b[0m.\n",
      "\n",
      "Successfully written \u001b[1m\u001b[36mhours\u001b[0m with shape (160,) and dtype 'float32' to group \u001b[1m\u001b[95mnoise\u001b[0m.\n",
      "\n",
      "Successfully written \u001b[1m\u001b[36mhours\u001b[0m with shape (480,) and dtype 'float32' to group \u001b[1m\u001b[95mtestpulses\u001b[0m.\n",
      "\n",
      "DataHandler Instance created.\n"
     ]
    }
   ],
   "source": [
    "ai.data.combine_h5(fname=\"combined_file-P_Ch0-L_Ch1\",\n",
    "                   files=[fn+'-P_Ch0-L_Ch1' for fn in file_names],\n",
    "                   src_dir=path_data,\n",
    "                   out_dir=path_data,\n",
    "                   groups_combine=[\"events\", \"testpulses\", \"controlpulses\", \"noise\"]\n",
    "                  )\n",
    "dh_combined = ai.DataHandler(channels=[0, 1], record_length=16384, sample_frequency=25000)\n",
    "dh_combined.set_filepath(path_h5=path_data, fname=\"combined_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need the events again at a later point, we can  include them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-10T11:04:18.082370Z",
     "start_time": "2021-11-10T11:04:17.413507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler Instance created.\n",
      "Accessing RDT File ...\n",
      "Total Records in File:  800\n",
      "Event Counts:  400\n",
      "Adding 80 triggered Events.\n",
      "Adding 80 Noise Events.\n",
      "Adding 240 Testpulse Events.\n",
      "Done.\n",
      "DataHandler Instance created.\n",
      "Accessing RDT File ...\n",
      "Total Records in File:  800\n",
      "Event Counts:  400\n",
      "Adding 80 triggered Events.\n",
      "Adding 80 Noise Events.\n",
      "Adding 240 Testpulse Events.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for fn in file_names:\n",
    "    dh = ai.DataHandler(channels=[0, 1], record_length=16384, sample_frequency=25000)\n",
    "    dh.set_filepath(path_h5=path_data, fname=fn)\n",
    "    dh.include_rdt(\n",
    "        path_data=path_data, \n",
    "        fname=fn, \n",
    "        ints_in_header=7,\n",
    "        tpa_list=[0, 1, -1],\n",
    "        event_dtype='float32',\n",
    "        lazy_loading=True,\n",
    "        origin=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you also have to update the combined file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file 'test_data/combined_file-P_Ch0-L_Ch1.h5'.\n",
      "Successfully combined files ['mock_001-P_Ch0-L_Ch1', 'mock_002-P_Ch0-L_Ch1'] into 'test_data/combined_file-P_Ch0-L_Ch1.h5' (18.0 KiB).\n",
      "Calculating extended \u001b[1m\u001b[36mhours\u001b[0m for all groups with datasets \u001b[1m\u001b[36mevent\u001b[0m, \u001b[1m\u001b[36mhours\u001b[0m, \u001b[1m\u001b[36mtime_s\u001b[0m, \u001b[1m\u001b[36mtime_mus\u001b[0m:\n",
      "DataHandler Instance created.\n",
      "Successfully written \u001b[1m\u001b[36mhours\u001b[0m with shape (160,) and dtype 'float32' to group \u001b[1m\u001b[95mevents\u001b[0m.\n",
      "\n",
      "Successfully written \u001b[1m\u001b[36mhours\u001b[0m with shape (160,) and dtype 'float32' to group \u001b[1m\u001b[95mnoise\u001b[0m.\n",
      "\n",
      "Successfully written \u001b[1m\u001b[36mhours\u001b[0m with shape (480,) and dtype 'float32' to group \u001b[1m\u001b[95mtestpulses\u001b[0m.\n",
      "\n",
      "DataHandler Instance created.\n"
     ]
    }
   ],
   "source": [
    "ai.data.combine_h5(fname=\"combined_file-P_Ch0-L_Ch1\",\n",
    "                   files=[fn+'-P_Ch0-L_Ch1' for fn in file_names],\n",
    "                   src_dir=path_data,\n",
    "                   out_dir=path_data,\n",
    "                   groups_combine=[\"events\", \"testpulses\", \"controlpulses\", \"noise\"]\n",
    "                  )\n",
    "dh_combined = ai.DataHandler(channels=[0, 1], record_length=16384, sample_frequency=25000)\n",
    "dh_combined.set_filepath(path_h5=path_data, fname=\"combined_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please forward questions and correspondence about this notebook to felix.wagner(at)oeaw.ac.at."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_cait",
   "language": "python",
   "name": "venv_cait"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
