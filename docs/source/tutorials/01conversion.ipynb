{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************************************\n",
    "# Conversion of Hardware-triggered Data\n",
    "****************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we generate mock hardware triggered data to test all functionality of Cait. The generated data is in all properties similar to data from the CRESST and COSINUS data aquisitions that work with the program CSS. The only exception is that the noise and pulses are not measured, but generated with parametric descriptions of the pulse shape and normal distributed noise.\n",
    "\n",
    "We start with importing the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:30:00.744811Z",
     "start_time": "2021-07-10T14:29:55.085793Z"
    }
   },
   "outputs": [],
   "source": [
    "import cait as ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TestData class handles the generation of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:30:02.924761Z",
     "start_time": "2021-07-10T14:30:00.748069Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = ai.data.TestData(filepath='test_data/mock_001', duration=1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate an RDT file, that holds all triggered events, test pulses and noise events. Right after the generation we call a check function, that prints the content of the first event, to check if the file is properly written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:31:09.216358Z",
     "start_time": "2021-07-10T14:31:08.344264Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rdt file written.\n",
      "DataHandler Instance created.\n",
      "#############################################################\n",
      "EVENT NUMBER:  0\n",
      "detector number (starting at 0):  0\n",
      "number of coincident pulses in digitizer module:  0\n",
      "module trigger counter (starts at 0, when TRA or WRITE starts):  1\n",
      "channel trigger delay relative to time stamp [Âµs]:  0\n",
      "absolute time [s] (computer time timeval.tv_sec):  1602879726\n",
      "absolute time [us] (computer time timeval.tv_us):  0\n",
      "Delay of channel trigger to testpulse [us]:  0\n",
      "time stamp of module trigger low word (10 MHz clock, 0 @ START WRITE ):  0\n",
      "time stamp of module trigger high word (10 MHz clock, 0 @ START WRITE ):  6\n",
      "number of qdc events accumulated until digitizer trigger:  0\n",
      "measuring hours (0 @ START WRITE):  0.0016666667070239782\n",
      "accumulated dead time of channel [s] (0 @ START WRITE):  0.0\n",
      "test pulse amplitude (0. for pulses, (0.,10.] for test pulses, >10. for control pulses):  0.10000000149011612\n",
      "DAC output of control program (proportional to heater power):  0.0\n"
     ]
    }
   ],
   "source": [
    "test_data._generate_rdt_file()\n",
    "dh = ai.DataHandler(nmbr_channels=2)\n",
    "dh.checkout_rdt(path_rdt='test_data/mock_001.rdt', read_events=1, verb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CON file holds the time stamps and pulse heights of the control pulses. Also for the CON file, we call a check function after the data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:31:18.049779Z",
     "start_time": "2021-07-10T14:31:18.003269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con file written.\n",
      "DataHandler Instance created.\n",
      "5 control pulses read from CON file.\n",
      " \tdetector_nmbr,\t \tpulse_height, \ttime_stamp_low, \ttime_stamp_high, \tdead_time, \tmus_since_last_tp\n",
      "1\t0\t\t6.1\t\t30000000\t\t0\t\t\t0.0\t[0]\n",
      "2\t1\t\t4.04\t\t30000000\t\t0\t\t\t0.0\t[0]\n",
      "3\t0\t\t6.05\t\t120000000\t\t0\t\t\t0.0\t[0]\n",
      "4\t1\t\t3.99\t\t120000000\t\t0\t\t\t0.0\t[0]\n",
      "5\t0\t\t5.97\t\t210000000\t\t0\t\t\t0.0\t[0]\n"
     ]
    }
   ],
   "source": [
    "test_data._generate_con_file()\n",
    "dh = ai.DataHandler(nmbr_channels=2)\n",
    "dh.checkout_con(path_con='test_data/mock_001.con', read_events=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To every RDT file belongs a PAR file, which is a text file with additional information. The generated PAR file can be checked by opening it with a text editor or with \"vim FILE_NAME\" in the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:31:21.916875Z",
     "start_time": "2021-07-10T14:31:21.910991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par file written.\n"
     ]
    }
   ],
   "source": [
    "test_data._generate_par_file()\n",
    "# test by looking at the text file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the data generation for a second file, this time we call a pre-implemented method that does the steps from above all at once. Notice that we specify the gap in measuring time between the two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:31:27.266613Z",
     "start_time": "2021-07-10T14:31:26.349874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rdt file written.\n",
      "Con file written.\n",
      "Par file written.\n"
     ]
    }
   ],
   "source": [
    "test_data.update_filepath(file_path='test_data/mock_002')\n",
    "test_data.generate(start_offset=1.5 * 3600, source='hw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cait library accesses and stores its data in HDF5 files, which are a structured file format and convenient for high-level applications. We are aware, that saving the data twice might be inefficient in terms of storage space. However, as a solution for this we propose to keep the raw data events only so long in the HDF5 files, until all needed high level features of the raw data are calculated. We show below how this is done.\n",
    "\n",
    "But first, we generate a HDF5 file from the events that are contained in the RDT file and the control pulses from the CON file. For this, the PAR file must be in the same directory as the RDT file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:31:31.080686Z",
     "start_time": "2021-07-10T14:31:31.077052Z"
    }
   },
   "outputs": [],
   "source": [
    "path_data = 'test_data/'\n",
    "file_names = ['mock_001',\n",
    "              'mock_002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:32:06.531969Z",
     "start_time": "2021-07-10T14:32:05.375586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler Instance created.\n",
      "Start converting.\n",
      "\n",
      "READ EVENTS FROM RDT FILE.\n",
      "Total Records in File:  800\n",
      "Getting good idx. (Depending on OS and drive reading speed, this might take some minutes!)\n",
      "Event Counts Channel 0: 400\n",
      "Event Counts Channel 1: 400\n",
      "Getting good tpas.\n",
      "Good consecutive counts: 400\n",
      "\n",
      "WORKING ON EVENTS WITH TPA = 0.\n",
      "CREATE DATASET WITH EVENTS.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc3516979474969b7d0440efccf5dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=160.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "WORKING ON EVENTS WITH TPA = -1.\n",
      "CREATE DATASET WITH NOISE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fc89017be641c9a5eba62cf1cda60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=160.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "WORKING ON EVENTS WITH TPA > 0.\n",
      "CREATE DATASET WITH TESTPULSES.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9130ba3a75245c6aac0db27318d46a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=480.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hdf5 dataset created in  test_data/\n",
      "Filepath and -name saved.\n",
      "Accessing CON File...\n",
      "200 Control Pulses for channel 0 in file.\n",
      "CON File included.\n",
      "DataHandler Instance created.\n",
      "Start converting.\n",
      "\n",
      "READ EVENTS FROM RDT FILE.\n",
      "Total Records in File:  800\n",
      "Getting good idx. (Depending on OS and drive reading speed, this might take some minutes!)\n",
      "Event Counts Channel 0: 400\n",
      "Event Counts Channel 1: 400\n",
      "Getting good tpas.\n",
      "Good consecutive counts: 400\n",
      "\n",
      "WORKING ON EVENTS WITH TPA = 0.\n",
      "CREATE DATASET WITH EVENTS.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8969e07d120493e9d1797a5e2777d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=160.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "WORKING ON EVENTS WITH TPA = -1.\n",
      "CREATE DATASET WITH NOISE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e68c978dd248f880263658c88c1a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=160.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "WORKING ON EVENTS WITH TPA > 0.\n",
      "CREATE DATASET WITH TESTPULSES.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597e16ffd51541ffbfb7f4b3c87d75b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=480.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hdf5 dataset created in  test_data/\n",
      "Filepath and -name saved.\n",
      "Accessing CON File...\n",
      "200 Control Pulses for channel 0 in file.\n",
      "CON File included.\n"
     ]
    }
   ],
   "source": [
    "# Conversion from Rdt to HDF5\n",
    "\n",
    "for file in file_names:\n",
    "    # --------------------------------------------------\n",
    "    # Convert Rdt to H5\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    dh = ai.DataHandler(channels=[0,1],\n",
    "                        record_length=16384,\n",
    "                        sample_frequency=25000)\n",
    "    \n",
    "    dh.convert_dataset(\n",
    "        path_rdt=path_data,\n",
    "        fname=file,\n",
    "        path_h5=path_data,\n",
    "        tpa_list=[0, 1, -1],\n",
    "        calc_mp=False,\n",
    "        calc_sev=False,\n",
    "        calc_nps=False,\n",
    "        lazy_loading=True,\n",
    "        event_dtype='float32',\n",
    "        ints_in_header=7,\n",
    "        memsafe=True,\n",
    "        dvm_channels=0,\n",
    "        batch_size=1000,\n",
    "        trace=False,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Include con file\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    dh.include_con_file(path_con_file=path_data + file + '.con')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we called for the first time the DataHandler class. This is a heavy class, that handles all the feature calculations of the raw data. It has stored the path to the HDF5 file as an attribute and saves all calculated properties there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are often in the position, that we want to process data from multiple consecutive measurements in common. For this, we can merge two converted files and specify, if we want to keep the indicidual files. For large scale data processing, were often events from more than a hundred RDT files is processed, we propose to call the conversion and merge function in a loop, while always deleting the files that are merged already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:32:13.699462Z",
     "start_time": "2021-07-10T14:32:13.489759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge done.\n"
     ]
    }
   ],
   "source": [
    "ai.data.merge_h5_sets(path_h5_a=path_data + file_names[0] + '-P_Ch0-L_Ch1.h5', \n",
    "                      path_h5_b=path_data + file_names[1] + '-P_Ch0-L_Ch1.h5', \n",
    "                      path_h5_merged=path_data + 'test_001.h5', \n",
    "                      continue_hours=True,\n",
    "                      keep_original_files=True,\n",
    "                      a_name='mock_001',\n",
    "                      b_name='mock_002',\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we kept the original two files, but we don't need the raw events from both anymore. So we delete them from one of the H5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:32:15.736869Z",
     "start_time": "2021-07-10T14:32:15.726548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler Instance created.\n",
      "Dataset Event deleted from group events.\n"
     ]
    }
   ],
   "source": [
    "dh = ai.DataHandler(\n",
    "    channels = [0, 1],\n",
    "    record_length = 16384,\n",
    "    sample_frequency = 25000,\n",
    "    )\n",
    "dh.set_filepath(path_h5=path_data, \n",
    "                fname=file_names[1], \n",
    "                appendix=True)\n",
    "dh.drop_raw_data(type=\"events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When displaying the content of the file, we see that there is no \"event\" data set in the \"events\" group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:32:17.856796Z",
     "start_time": "2021-07-10T14:32:17.841477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following properties are in the HDF5 sets can be accessed through the get(group, dataset) methode.\n",
      "The following data sets are contained in the the group controlpulses:\n",
      "dataset: hours, shape: (200,)\n",
      "dataset: pulse_height, shape: (2, 200)\n",
      "The following data sets are contained in the the group events:\n",
      "dataset: hours, shape: (80,)\n",
      "dataset: time_mus, shape: (80,)\n",
      "dataset: time_s, shape: (80,)\n",
      "The following data sets are contained in the the group noise:\n",
      "dataset: event, shape: (2, 80, 16384)\n",
      "dataset: hours, shape: (80,)\n",
      "dataset: time_mus, shape: (80,)\n",
      "dataset: time_s, shape: (80,)\n",
      "The following data sets are contained in the the group testpulses:\n",
      "dataset: event, shape: (2, 240, 16384)\n",
      "dataset: hours, shape: (240,)\n",
      "dataset: testpulseamplitude, shape: (240,)\n",
      "dataset: time_mus, shape: (240,)\n",
      "dataset: time_s, shape: (240,)\n"
     ]
    }
   ],
   "source": [
    "dh.content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need them again at a later point, we can again include them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:32:21.107202Z",
     "start_time": "2021-07-10T14:32:20.576291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing RDT File ...\n",
      "Total Records in File:  800\n",
      "Event Counts:  399\n",
      "Adding 80 triggered Events.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "dh.include_rdt(\n",
    "    path_data=path_data, \n",
    "    fname=file_names[1], \n",
    "    ints_in_header=7,\n",
    "    tpa_list=[0, 1, -1],\n",
    "    event_dtype='float32',\n",
    "    lazy_loading=True,\n",
    "    origin=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same works for data sets that are merged from multiple rdt files. For this we need the origin data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:32:23.309876Z",
     "start_time": "2021-07-10T14:32:23.302424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler Instance created.\n"
     ]
    }
   ],
   "source": [
    "path_data = 'test_data/'\n",
    "fname = 'test_001'\n",
    "channels_rdt = [0,1]\n",
    "dh = ai.DataHandler(channels=channels_rdt)\n",
    "dh.set_filepath(path_h5=path_data,\n",
    "                fname=fname,\n",
    "                appendix=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:32:25.873281Z",
     "start_time": "2021-07-10T14:32:25.866088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Event deleted from group events.\n"
     ]
    }
   ],
   "source": [
    "dh.drop_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:32:34.626308Z",
     "start_time": "2021-07-10T14:32:34.256655Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing RDT File ...\n",
      "Total Records in File:  800\n",
      "Event Counts:  399\n",
      "Adding 80 triggered Events.\n",
      "Done.\n",
      "Accessing RDT File ...\n",
      "Total Records in File:  800\n",
      "Event Counts:  399\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for file in file_names:\n",
    "    dh.include_rdt(path_data=path_data, \n",
    "                   fname=file, \n",
    "                   channels=[0, 1], \n",
    "                   origin=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:32:36.942298Z",
     "start_time": "2021-07-10T14:32:36.927503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following properties are in the HDF5 sets can be accessed through the get(group, dataset) methode.\n",
      "The following data sets are contained in the the group controlpulses:\n",
      "dataset: hours, shape: (400,)\n",
      "dataset: pulse_height, shape: (2, 400)\n",
      "The following data sets are contained in the the group events:\n",
      "dataset: event, shape: (2, 160, 16384)\n",
      "dataset: hours, shape: (160,)\n",
      "dataset: origin, shape: (160,)\n",
      "dataset: time_mus, shape: (160,)\n",
      "dataset: time_s, shape: (160,)\n",
      "The following data sets are contained in the the group noise:\n",
      "dataset: event, shape: (2, 160, 16384)\n",
      "dataset: hours, shape: (160,)\n",
      "dataset: origin, shape: (160,)\n",
      "dataset: time_mus, shape: (160,)\n",
      "dataset: time_s, shape: (160,)\n",
      "The following data sets are contained in the the group testpulses:\n",
      "dataset: event, shape: (2, 480, 16384)\n",
      "dataset: hours, shape: (480,)\n",
      "dataset: origin, shape: (480,)\n",
      "dataset: testpulseamplitude, shape: (480,)\n",
      "dataset: time_mus, shape: (480,)\n",
      "dataset: time_s, shape: (480,)\n"
     ]
    }
   ],
   "source": [
    "dh.content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please forward questions and correspondence about this notebook to felix.wagner(at)oeaw.ac.at."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
