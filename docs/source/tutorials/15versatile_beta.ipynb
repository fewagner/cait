{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd31f1d-ecea-49dc-a763-a327695775c0",
   "metadata": {},
   "source": [
    "# `cait.versatile` preview (experimental)\n",
    "In this notebook, we present some of the upcoming functionalities of `cait`. Do not yet use those functions in production as they are subject to change. As of now, they are just presented to give a first taste for what's coming and to let you play around with it already (and maybe give feedback about possible problems you run into)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d765590e-0732-4660-8a91-ba9fc061baeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler Instance created.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../testdata/to_merge2.h5'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cait as ai\n",
    "import cait.versatile as vai\n",
    "import numpy as np\n",
    "\n",
    "# We use the hdf5 file as created in the previous tutorial\n",
    "dh = ai.DataHandler(channels=[0,1])\n",
    "dh.set_filepath(path_h5='../testdata', fname='mock_001', appendix=False)\n",
    "\n",
    "# We also copy it for later use\n",
    "import shutil # just used to create a quick copy of our already existing mock data file\n",
    "import os     # just used to create a quick copy of our already existing mock data file\n",
    "\n",
    "shutil.copy(dh.get_filepath(), os.path.join(dh.get_filedirectory(),'to_merge1.h5'))\n",
    "shutil.copy(dh.get_filepath(), os.path.join(dh.get_filedirectory(),'to_merge2.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0693b4-a953-4f6c-9b0c-b0f19544737a",
   "metadata": {},
   "source": [
    "## Combining/Merging HDF5 files for simultaneous analysis\n",
    "The HDF5 library introduced virtual datasets which is now also exploited in `cait` to easily combine files and analyse them together. A new HDF5 file will be created but its contents are merely links to the already existing 'source files'. Once they are combined, you can load the new HDF5 file like any other file and start analysis. Note that since the data is still stored in the original files, you may not move/delete them. \n",
    "If you wish to actually merge the files (and copy all the data into a single file) you can do so using the `merge` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f0805c0-588a-4d90-9b97-cbbe8b18fecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file '../testdata/merged_file.h5'.\n",
      "Successfully combined files ['to_merge1', 'to_merge2'] into '../testdata/merged_file.h5' (19.6 KiB).\n"
     ]
    }
   ],
   "source": [
    "vai.combine(fname=\"merged_file\",                   # output name\n",
    "            files=[\"to_merge1\", \"to_merge2\"],      # files to merge/combine\n",
    "            src_dir=\"../testdata\",                 # folder for input files\n",
    "            out_dir=\"../testdata\",                 # folder for output file\n",
    "            groups_combine=[\"testpulses\",\"noise\"], # the groups in the HDF5 file you want to combine\n",
    "                                                # (have to be present in ALL source files)\n",
    "            groups_include=[])                     # the groups you want to include additionally\n",
    "                                                # (have to be present in at least one source file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a67d2ef-ec5e-45e0-bf20-2931b10ad059",
   "metadata": {},
   "source": [
    "As you can see, the resulting file size is only a few KiB due to no data being copied. We can now create a `DataHandler` to the combined file and inspect it with our usual methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d815b2d6-6998-4111-93d6-73cd186c84e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler Instance created.\n",
      "DataHandler linked to HDF5 file '../testdata/merged_file.h5'\n",
      "HDF5 file size on disk: 30.0 KiB\n",
      "Groups in file: ['noise', 'testpulses'].\n",
      "\n",
      "The HDF5 file contains virtual datasets linked to the following files: {'../testdata/to_merge1.h5', '../testdata/to_merge2.h5'}\n",
      "All of the external sources are currently available.\n",
      "\n",
      "Time between first and last testpulse: 2.78 h\n",
      "First testpulse on/at: 2020-10-16 20:22:06+00:00 (UTC)\n",
      "Last testpulse on/at: 2020-10-16 23:08:36+00:00 (UTC)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dh_combined = ai.DataHandler(channels=[0,1])\n",
    "dh_combined.set_filepath(path_h5='../testdata', fname='merged_file', appendix=False)\n",
    "\n",
    "print(dh_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24dcbeb7-8511-4510-a9fd-6aada8ea9765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95mtestpulses\u001b[0m\n",
      "  \u001b[1m\u001b[36madd_mainpar            \u001b[0m\u001b[93m (v)\u001b[0m (2, 2666, 16)     float64\n",
      "  |array_max                  (2, 2666)\n",
      "  |array_min                  (2, 2666)\n",
      "  |var_first_eight            (2, 2666)\n",
      "  |mean_first_eight           (2, 2666)\n",
      "  |var_last_eight             (2, 2666)\n",
      "  |mean_last_eight            (2, 2666)\n",
      "  |var                        (2, 2666)\n",
      "  |mean                       (2, 2666)\n",
      "  |skewness                   (2, 2666)\n",
      "  |max_derivative             (2, 2666)\n",
      "  |ind_max_derivative         (2, 2666)\n",
      "  |min_derivative             (2, 2666)\n",
      "  |ind_min_derivative         (2, 2666)\n",
      "  |max_filtered               (2, 2666)\n",
      "  |ind_max_filtered           (2, 2666)\n",
      "  |skewness_filtered_peak     (2, 2666)\n",
      "  \u001b[1m\u001b[36mcuts_SEV               \u001b[0m\u001b[93m (v)\u001b[0m (2, 2666)         bool\n",
      "  \u001b[1m\u001b[36mdac_output             \u001b[0m\u001b[93m (v)\u001b[0m (2666,)           float64\n",
      "  \u001b[1m\u001b[36mevent                  \u001b[0m\u001b[93m (v)\u001b[0m (2, 2666, 16384)  float32\n",
      "  \u001b[1m\u001b[36mhours                  \u001b[0m\u001b[93m    \u001b[0m (2666,)           float32\n",
      "  \u001b[1m\u001b[36mmainpar                \u001b[0m\u001b[93m (v)\u001b[0m (2, 2666, 10)     float64\n",
      "  |pulse_height               (2, 2666)\n",
      "  |onset                      (2, 2666)\n",
      "  |rise_time                  (2, 2666)\n",
      "  |decay_time                 (2, 2666)\n",
      "  |slope                      (2, 2666)\n",
      "  \u001b[1m\u001b[36mof_ph                  \u001b[0m\u001b[93m (v)\u001b[0m (2, 2666)         float64\n",
      "  \u001b[1m\u001b[36mtestpulseamplitude     \u001b[0m\u001b[93m (v)\u001b[0m (2666,)           float64\n",
      "  \u001b[1m\u001b[36mtime_mus               \u001b[0m\u001b[93m (v)\u001b[0m (2666,)           int32\n",
      "  \u001b[1m\u001b[36mtime_s                 \u001b[0m\u001b[93m (v)\u001b[0m (2666,)           int32\n"
     ]
    }
   ],
   "source": [
    "dh_combined.content(\"testpulses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfee8d-1b46-4745-8f64-2debc50700ab",
   "metadata": {},
   "source": [
    "We see that `print(dh_combined)` tells us which files are linked to the HDF5 file of our `DataHandler` object. In principle it could happend that some of these source files get deleted. In this case, you could still create the `DataHandler` but subsequent calculations can have unexpected behaviour. This is why `print(dh_combined)` also tells you whether all sources are available or not.\n",
    "\n",
    "Using `content` now also changed slightly as it shows you an indicator next to datasets which are virtual.\n",
    "\n",
    "In principle, this is all the difference there is. You can do analysis with `dh_combined` in the same way you would do with `dh`. The only thing worth mentioning is that *writing* to virtual datasets is special. Depending on your use case you could either want to change the data in all source files (which could lead to confusion and unexpected results), or you could want to detach the virtual dataset and write to a regular dataset of that name. In most cases, the latter will probably be the desired behaviour. As a safety feature, the `set` method checks whether you are attempting to write to a virtual dataset or not and you *have* to make this decision upon calling it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9293307-5c8f-404c-aa15-2ae3028bc187",
   "metadata": {},
   "source": [
    "**To actually merge** the files, use `cait.versatile.merge` instead of `cait.versatile.combine`.\n",
    "\n",
    "**Note on continuous time data:** The methods described above only stick files together, they do *not* perform any calculations such as creating a continuous time dataset. Usually, such things can be readily calculated, though. In this example, we could do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae713533-c982-4f2d-b24b-dba44f331924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully written \u001b[1m\u001b[36mhours\u001b[0m with shape (2666,) and dtype 'float32' to group \u001b[1m\u001b[95mtestpulses\u001b[0m.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestamp_s = dh_combined.get(\"testpulses\", \"time_s\")     # second timestamps\n",
    "microseconds = dh_combined.get(\"testpulses\", \"time_mus\")  # microsecond timestamps\n",
    "\n",
    "# Get the earliest timestamp (and set it to 0 hours later. Adjust if you need anything else.)\n",
    "earliest_ind = np.argmin(timestamp_s)\n",
    "start_timestamp_s = timestamp_s[earliest_ind]\n",
    "start_microseconds = microseconds[earliest_ind]\n",
    "\n",
    "t = ((timestamp_s + microseconds/1e6) - (start_timestamp_s + start_microseconds/1e6) )/3600\n",
    "\n",
    "dh_combined.set(\"testpulses\", hours=t, write_to_virtual=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab733be8-ffa1-4505-9f17-3e194c35c313",
   "metadata": {},
   "source": [
    "In the last step, we have written the hours dataset into the HDF5 file. Notice that we did *not* save it to the original files by setting `write_to_virtual=False`. This new dataset is now really saved in `dh_combined`.\n",
    "\n",
    "At some point, a timestamp conversion function like the above will be available in `cait.versatile`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b6020-602b-4df0-861b-5e45bf139909",
   "metadata": {},
   "source": [
    "## Inspecting stream data using `cait.versatile.StreamViewer`\n",
    "You can load the stream file and use the buttons to navigate backwards/forward in time. Using the `Data Info` button, you can display useful information about the data on screen. The most helpful ones being the `sigma_y` (standard deviation of data in y-direction) and `delta_y` (difference of maximum and minimum of data in y-direction). To calculate those values, the lines that are currently *on-screen* and *visible* are considered, i.e. if you want to deduce the baseline noise's standard deviation, you just have to zoom into a baseline such that nothing else is on screen and hit `Data Info`.\n",
    "\n",
    "Note that currently only VDAQ2 stream files are supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0327ba01-4571-4449-8bdd-6cfb02e33f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = '/path/to/stream/file.bin'\n",
    "\n",
    "vai.StreamViewer(hardware=\"vdaq2\", file=fpath, template=\"plotly_dark\", width=1000, downsample_factor=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8363e292-44ea-4392-9fd1-d1facf0f7659",
   "metadata": {},
   "source": [
    "![](media/streamViewer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c4baa-b6b7-453d-b322-cb872646b8d6",
   "metadata": {},
   "source": [
    "## Iterators and `cait.versatile.apply`\n",
    "The idea of iterators is to streamline the application of functions to all events in an HDF5 file. Using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fd6240e-2637-48f9-92e3-6a214437b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "it100 = dh.get_event_iterator(group=\"events\", channel=0, batch_size=100)\n",
    "it1 = dh.get_event_iterator(group=\"events\", channel=0, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea39509-a0c5-4236-b286-2c74f1b61821",
   "metadata": {},
   "source": [
    "we can get an iterator object `it` which iterates over the events of channel 0 in the group 'events'. It does so with a batch size of 100. In principle, we can set the batch size to 1 but reading data in batches can be significantly faster as it needs less accesses to the HDF5 file on disk.\n",
    "The best way to use an iterator is within a context, because this way, the HDF5 file is kept open (and the access speed is hence way faster):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7b17fea-3f01-4ee8-8ba9-920319dd437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using context manager\n",
    "with it1 as it:\n",
    "    for event in it:\n",
    "        np.max(event)\n",
    "        \n",
    "# Without context manager (slower)\n",
    "for event in it:\n",
    "        np.max(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df495f1-7725-493a-994d-8efe8c488379",
   "metadata": {},
   "source": [
    "We can also create iterators of only a subset of events using flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "975b5d20-786b-48ab-badc-97c5217cf5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = dh.get(\"events\", \"pulse_height\", 0) > 5\n",
    "it_flag = dh.get_event_iterator(group=\"events\", channel=0, flag=flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634bcaea-82a3-4537-9026-bacc90a76acc",
   "metadata": {},
   "source": [
    "Using the `apply` function, we can apply a function to all events in such an iterator. The possiblity of multiprocessing is already built-in (notice that locally defined functions can not be used, though. To work around this, just define them in a scrip.py and load them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9428a8c-29cb-4327-a429-82a730d051ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64378a73c74e4d0283d1391b0415b5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([5.1342773, 5.3463745, 5.9292603, 5.4537964, 5.415039 , 5.373535 ,\n",
       "       5.8517456, 5.83313  ], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = vai.apply(np.max, it_flag)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1038a6-845e-4fd5-8beb-f2ca13d701a7",
   "metadata": {},
   "source": [
    "# Stay tuned for more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70259c-6db9-4607-b029-81aece20148e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_cait",
   "language": "python",
   "name": "venv_cait"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
