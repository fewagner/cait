{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAQ Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an analysis of the hardware-triggered data of a detector module. We calculate standard events, noise power spectra, optimum filter, resolutions and trigger thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T22:50:32.553229Z",
     "start_time": "2021-07-04T22:50:32.549907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let´s start!\n"
     ]
    }
   ],
   "source": [
    "print('Let´s start!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the notebook, we build a data set of hardware triggered files, with which we create a standard event, a noise power spectrum and an optimum filter. Afterwards we trigger the corresponding stream files and extract descriptive features. Finally we do cuts and an energy calibration on the triggered and processed events and extract a histogram of the recoil energies and a light yield plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cait as ai\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tracemalloc\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "%config InlineBackend.figure_formats = ['svg']  # we need this for a suitable resolution of the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we define a set of constants and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = ... # put an string for the number of the experiments run, e.g. '34'\n",
    "MODULE = ...  # put a name for the detector, e.g. 'DetA'\n",
    "PATH_HW_DATA = ...  # path to the directory in which the RDT and CON files are stored\n",
    "PATH_PROC_DATA = ...  # path to where you want to store the HDF5 files\n",
    "FILE_NMBRS = []  # a list of string, the file number you want to analyse, e.g. ['001', '002', '003']\n",
    "RDT_CHANNELS = []  # a list of strings of the channels, e.g. [0, 1] (written in PAR file - attention, the PAR file counts from 1, Cait from 0)\n",
    "RECORD_LENGTH = 16384  # the number of samples within a record window  (read in PAR file)\n",
    "SAMPLE_FREQUENCY = 25000  # the sample frequency of the measurement (read in PAR file)\n",
    "DOWN_SEF = 4  # the downsample rate for the standard event fit\n",
    "DOWN_BLF = 16  # the downsample rate for the baseline fit\n",
    "PROCESSES = 8  # the number of processes for parallelization\n",
    "PCA_COMPONENTS = 2  # the number of pca components to calculate\n",
    "SKIP_FNMR = []    # in case the loop crashed at some point and you want to start from a specific file number, write here the numbers to ignore, e.g. ['001', '002']\n",
    "ALLOWED_NOISE_TRIGGERS = 1  # the number of noise triggers we allow for in threshold calculations\n",
    "\n",
    "# typically you need not change the values below this line!\n",
    "\n",
    "FNAME_RESOLUTION = 'resolution_001'  # file anticipated file name for the resolution data set\n",
    "FNAME_EFFICIENCY = 'efficiency_001'  # file anticipated file name for the efficiency data set\n",
    "FNAME_TRAINING = 'training_001'  # file anticipated file name for the training data set\n",
    "FNAME_HW = 'hw_{:03d}'.format(len(FILE_NMBRS) - 1)\n",
    "H5_CHANNELS = list(range(len(RDT_CHANNELS)))\n",
    "SEF_APP = '_down{}'.format(DOWN_SEF) if DOWN_SEF > 1 else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assamble calculated values from further down in the notebook. Fill them up while you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTOR_MASS = ... # the detector mass in kg\n",
    "print('Detector mass in kg: ', DETECTOR_MASS)\n",
    "BL_RESOLUTION_OF = []  # list of the baseline resolutions, calculated with the superposition method, in mV\n",
    "THRESHOLDS = [(6.5 * r) * 1e-3 for r in BL_RESOLUTION_OF]\n",
    "print('OF resolution in V: ', BL_RESOLUTION_OF)\n",
    "print('OF thresholds in V: ', THRESHOLDS)\n",
    "FIT_BL_SIGMAS = []  # list of the baseline resolutions, calculated with the noise fit model, in V\n",
    "FIT_THRESHOLDS = []  # list of the trigger thresholds, calculated with the noise fit model, in V\n",
    "print('Fit thresholds in V: ', FIT_THRESHOLDS)\n",
    "TRUNCATION_LEVELS = []  # list of the truncation levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Cait, most data processing happens with the DataHandler class of that we create an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw = ai.DataHandler(run=RUN,\n",
    "                       module=MODULE,\n",
    "                       channels=RDT_CHANNELS,\n",
    "                       sample_frequency=SAMPLE_FREQUENCY,\n",
    "                       record_length=RECORD_LENGTH)\n",
    "\n",
    "dh_hw.set_filepath(path_h5=PATH_PROC_DATA,\n",
    "                fname='hw_{:03d}'.format(len(FILE_NMBRS)-1),\n",
    "                appendix=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAQ Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with several analysis steps on the hardware data, to extract a proper filter and a threshold for filtering of the continuous stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the raw data events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei = ai.EventInterface(module=MODULE,\n",
    "                       run=RUN,\n",
    "                       nmbr_channels=len(H5_CHANNELS),\n",
    "                       sample_frequency=SAMPLE_FREQUENCY,\n",
    "                       record_length=RECORD_LENGTH)\n",
    "ei.load_h5(path=PATH_PROC_DATA, \n",
    "           fname=FNAME_HW, \n",
    "           channels=RDT_CHANNELS, \n",
    "           appendix=False, \n",
    "           which_to_label=['events'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can viewing events, we can also create a labels CSV file to store labels for the raw data events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ei.create_labels_csv(path=PATH_PROC_DATA)\n",
    "# ei.load_labels_csv(path=PATH_PROC_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ei.load_of()  # this is only possible once the OF was calculated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now view events. Press 'o' for the options menu and a variety of additional plotting features. Some you can only use after having calculated the according properties. You can come back to these celss later, to use the additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei.start(start_from_idx=0, print_label_list=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEV, NPS, OF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is calculating the main parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.calc_mp(type='events')\n",
    "dh_hw.calc_mp(type='testpulses')\n",
    "dh_hw.calc_mp(type='noise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use routines to watch the main parameter distribution or access them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.get('events', 'decay_time')  # returns array of the decay times of all channels and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = [(0, 40), (0, 40)]\n",
    "for c in H5_CHANNELS:\n",
    "    dh_hw.show_values(group='events', key='decay_time', bins=200, idx0=c,  range=ranges[c],\n",
    "                       xlabel='Decay Time (ms)', ylabel='Counts', title='Channel {}'.format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to preceed with calculating a noise power spectrum. For this we need clean baselines for calculation of noise power spectra and simulation of events, this we do with the fit error of a polynomial fit to the baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.calc_bl_coefficients(down=DOWN_BLF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the histogram of the fit error, we can see which is a suitable upper limit of the error for either of the channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in H5_CHANNELS:\n",
    "    dh_hw.show_values(group='noise', key='fit_rms', bins=200, idx0=c, range=(0,1e-5),\n",
    "                   xlabel='Baseline Fit RMS', ylabel='Counts', title='Channel {}'.format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the noise power spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.calc_nps(rms_cutoff=[3.5e-6, 7e-6], window=True)\n",
    "for c in H5_CHANNELS:\n",
    "    dh_hw.show_nps(channel=c, title='Channel {} NPS'.format(c), yran=(1e-2, 1e3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cuts we will need some knowledge about the pulse heights, so plot a first histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in H5_CHANNELS:\n",
    "    dh_hw.show_values(group='events', key='mainpar', bins=250, idx0=c, idx2=0, range=(0,2.4), yran=(0,300),\n",
    "                   xlabel='Pulse Height (V)', ylabel='Counts', title='Spectrum PH Channel {}'.format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid Squid jumps in the SEV, we want to do a cut on the slope of the events. For this, we plot the linear slope to determine cut values. The slope is defined as right - left baseline level. The left and right baseline levels are determined as the average of the first and last eight of the record window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in H5_CHANNELS:\n",
    "    dh_hw.show_values(group='events', key='slope', bins=200, idx0=c, yran=(0,100), range=(-1, 1),\n",
    "                   xlabel='$A_{R}-A_{L}$ (V)', ylabel='Counts', title='Linear Slope Channel {}'.format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make an informed decision about the cuts we put for the standard event calculation, lets plot as well the rise time and onset interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in H5_CHANNELS:\n",
    "    dh_hw.show_values(group='events', key='rise_time', bins=200, idx0=c, yran=(0,20), #range=(-1, 1),\n",
    "                   xlabel='Rise Time (ms)', ylabel='Counts', title='Channel {}'.format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in H5_CHANNELS:\n",
    "    dh_hw.show_values(group='events', key='onset', bins=200, idx0=c, yran=(0,100), #range=(-1, 1),\n",
    "                   xlabel='Onset (ms)', ylabel='Counts', title='Channel {}'.format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the standard events with suitable cut values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.calc_sev(pulse_height_interval=[[0.3, 0.5], [0.2, 0.65]],\n",
    "                left_right_cutoff=[0.5, 0.5],  # in V\n",
    "                rise_time_interval=[(0, 30), (0, 30)],  # in ms\n",
    "                decay_time_interval=[(6, 14), (2, 14)],  # in ms\n",
    "                onset_interval=[(-20, 20), (-5, 10)],  # in ms\n",
    "                t0_start=None,\n",
    "                opt_start=True,  # better fit, but much slower (~ minutes)\n",
    "                )\n",
    "for c in H5_CHANNELS:\n",
    "    dh_hw.show_sev(channel=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a test pulse standard event we need to know the typical pulse heights of test pulses, which we look up in the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = [(0, 1.6), (0, 0.3)]\n",
    "for c, r in zip(H5_CHANNELS, ranges):\n",
    "    dh_hw.show_values(group='testpulses', key='mainpar', bins=400, idx0=c, idx2=0, range=r,\n",
    "                   xlabel='Pulse Height (V)', ylabel='Counts', title='Testpulses PH {}'.format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these cut values we create test pulse standard events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.calc_sev(type='testpulses',\n",
    "            pulse_height_interval=[[0.3, 1], [0.05, 0.2]],\n",
    "            left_right_cutoff=[0.5, 0.5],\n",
    "            rise_time_interval=None,\n",
    "            decay_time_interval=None,\n",
    "            onset_interval=[[-10, 10], [-10, 10]],\n",
    "            #t0_start=(-1, 0),\n",
    "            opt_start=True)\n",
    "for c in H5_CHANNELS:\n",
    "    dh_hw.show_sev(name_appendix='_tp', channel=c, show_fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a filter to trigger the stream. The filter is the ratio of the time-inversed SEV and the NPS in Fourier space. We have all these requirements and can create the filter for events and testpulses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.calc_of()\n",
    "for c in H5_CHANNELS:\n",
    "    dh_hw.show_of(channel=c, yran=(1e-10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.calc_of(name_appendix='_tp')\n",
    "for c in H5_CHANNELS:\n",
    "    dh_hw.show_of(channel=c, group_name_appendix='_tp', yran=(1e-10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.apply_of()\n",
    "dh_hw.apply_of(type='testpulses', name_appendix_group='_tp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now drop the raw data events of the test pulses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dh_hw.drop_raw_data(type='testpulses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the SEV for Events and Particles and the NPS to xy files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(-RECORD_LENGTH/4, RECORD_LENGTH*3/4, 1)/SAMPLE_FREQUENCY\n",
    "\n",
    "for r, c in zip(RDT_CHANNELS, H5_CHANNELS):\n",
    "\n",
    "    ai.data.write_xy_file(filepath=PATH_PROC_DATA + 'xy_files/Channel_{}_SEV_Particle.xy'.format(r),\n",
    "                         data=[time, \n",
    "                               dh_hw.get('stdevent', 'event')[c]],\n",
    "                         title='Run {} Channel {} SEV Particle'.format(RUN, r),\n",
    "                         axis=['Time (ms)', \n",
    "                               'Amplitude (V)'])      \n",
    "\n",
    "    ai.data.write_xy_file(filepath=PATH_PROC_DATA + 'xy_files/Channel_{}_SEV_TP.xy'.format(r),\n",
    "                         data=[time, \n",
    "                               dh_hw.get('stdevent_tp', 'event')[c]],\n",
    "                         title='Run {} Channel {} SEV TP'.format(RUN, r),\n",
    "                         axis=['Time (ms)', \n",
    "                               'Amplitude (V)'])\n",
    "                               \n",
    "    ai.data.write_xy_file(filepath=PATH_PROC_DATA + 'xy_files/Channel_{}_NPS.xy'.format(r),\n",
    "                         data=[dh_hw.get('noise', 'freq'), \n",
    "                               dh_hw.get('noise', 'nps')[c]],\n",
    "                         title='Run {} Channel {} NPS'.format(RUN, r),\n",
    "                         axis=['Frequency (Hz)', \n",
    "                               'Amplitude (a.u.)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set a trigger threshold, we need the baseline resolution. This we determine by superposing the standard event to empty noise baselines and measuring the sigma of the, roughly Gaussian distributed, height reconstruction. Befor we start the simulation, we find the number of empty noise baselines in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.get('noise', 'hours').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.simulate_pulses(path_sim=PATH_PROC_DATA + FNAME_RESOLUTION + '.h5',\n",
    "                      size_events=3000,  # should be below Nmbr of clean baselines, otherwise activate reuse_bl\n",
    "                      reuse_bl=True,\n",
    "                      ev_discrete_phs=[[1], [1]],\n",
    "                      t0_interval=[-20, 20],  # in ms\n",
    "                      rms_thresholds=[4e-6, 8e-6],\n",
    "                      fake_noise=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simulated resolution data set, we determine the pulse height with the OF, the SEF and the raw pulse height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_res = ai.DataHandler(nmbr_channels=2)\n",
    "dh_res.set_filepath(path_h5=PATH_PROC_DATA, fname=FNAME_RESOLUTION, appendix=False)\n",
    "\n",
    "dh_res.apply_of()\n",
    "dh_res.calc_mp(type='events')\n",
    "dh_res.apply_sev_fit(type='events', down=DOWN_SEF, verb=True, t0_bounds=(-25, 25), processes=PROCESSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the reconstructed pulse height histograms, to check if they are Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in H5_CHANNELS:\n",
    "    dh_res.show_values(group='events', key='of_ph', bins=250, idx0=c, \n",
    "                   xlabel='Pulse Height (V)', ylabel='Counts', title='Channel {} Resolution OF'.format(c))\n",
    "    dh_res.show_values(group='events', key='mainpar', bins=250, idx0=c, idx2=0,\n",
    "                   xlabel='Pulse Height (V)', ylabel='Counts', title='Channel {} Resolution PH'.format(c))\n",
    "    dh_res.show_values(group='events', key='sev_fit_par', bins=250, idx0=c, idx2=0,\n",
    "                   xlabel='Pulse Height (V)', ylabel='Counts', title='Channel {} Resolution SEV Fit'.format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to calculate the resolutions, which are the sigmas of above Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolutions_of, mus_of = dh_res.calc_resolution(pec_factors=None, ph_intervals=[(0,2), (0,2)], \n",
    "                                      use_tp=False, of_filter=True, sev_fit=False, fit_gauss=True)\n",
    "resolutions_ph, mus_ph = dh_res.calc_resolution(pec_factors=None, ph_intervals=[(0,2), (0,2)], \n",
    "                                      use_tp=False, of_filter=False, sev_fit=False, fit_gauss=True)\n",
    "resolutions_fit, mus_fit = dh_res.calc_resolution(pec_factors=None, ph_intervals=[(0,2), (0,2)], \n",
    "                                      use_tp=False, of_filter=False, sev_fit=True, fit_gauss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the optimum filter, as estimator for the pulse height, is biased. We calculate the bias-correction factor as one over the mean of above Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OF_CORRECTION = [1/0.991, 1/0.975]\n",
    "print('These factors should be multiplied to OF outputs: ', OF_CORRECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Trigger Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the treshold with 1 noise trigger per kg days with a fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.apply_of(type='noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in H5_CHANNELS:\n",
    "    cut = ai.cuts.LogicalCut(dh_hw.get('noise', 'of_ph')[c] < 0.02)\n",
    "\n",
    "    dh_hw.estimate_trigger_threshold(channel=c,\n",
    "                                  detector_mass=DETECTOR_MASS,\n",
    "                                  allowed_noise_triggers=ALLOWED_NOISE_TRIGGERS,\n",
    "                                  cut_flag=cut.get_flag(),\n",
    "                                  ll=0,\n",
    "                                  ul=20,\n",
    "                                  yran=(0.1, 3e7),\n",
    "                                  xran_hist=(2, 9),\n",
    "                                  xran=(2, 14),\n",
    "                                  bins=250,\n",
    "                                  model='pollution_exponential'\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stream triggering, we want to apply a trucated standard event fit and therefore need to determine the truncation level. This we do with the reconstruction error of a principal component analysis (PCA), i.e. a singular value decomposition. First we apply several cuts to get only clean events for the PCA calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_events = ai.cuts.LogicalCut(initial_condition=np.abs(dh_hw.get('events', 'slope')[0]) < 0.2)\n",
    "clean_events.add_condition(np.abs(dh_hw.get('events', 'slope')[1]) < 0.2)\n",
    "clean_events.add_condition(dh_hw.get('events', 'pulse_height')[0] < 1) \n",
    "clean_events.add_condition(dh_hw.get('events', 'pulse_height')[1] < 1.5) \n",
    "clean_events.add_condition(dh_hw.get('events', 'onset')[0] < 20) \n",
    "clean_events.add_condition(dh_hw.get('events', 'onset')[0] > -20)\n",
    "clean_events.add_condition(dh_hw.get('events', 'onset')[1] < 20) \n",
    "clean_events.add_condition(dh_hw.get('events', 'onset')[1] > -20)\n",
    "print('Nmbr clean events: ', len(clean_events))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a suitable number of components for the PCA reconstruction and apply it only to the clean events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.apply_pca(nmbr_components=PCA_COMPONENTS, down=DOWN_SEF, fit_idx=clean_events.get_idx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and second principal components, i.e. eigenvectors, are the two most occuring, linearly independent, templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = dh_hw.get('events', 'pca_components')\n",
    "\n",
    "for c in H5_CHANNELS:\n",
    "    plt.close()\n",
    "    ai.styles.use_cait_style()\n",
    "    for i, comp in enumerate(components[c]):\n",
    "        plt.plot(comp, label='Component {}'.format(i+1))\n",
    "    plt.title('Principal Components Channel {}'.format(c))\n",
    "    ai.styles.make_grid()\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Amplitude (V)')\n",
    "    plt.xlim(3000, 7000)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine truncation levels, we plot the PCA reconstruction error vs. the pulse height. We typically see a pulse height, above which the error strongly increases. This is out truncation level for the channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ranges = [(0, 2), (0, 0.7)]\n",
    "y_ranges = [(0, 0.0001), (0, 0.0001)]\n",
    "\n",
    "for c, xr, yr in zip(H5_CHANNELS, x_ranges, y_ranges):\n",
    "    dh_hw.show_scatter(groups=['events', 'events'],\n",
    "                    keys=['mainpar', 'pca_error'],\n",
    "                    title=None,\n",
    "                    idx0s=[c, c],  # 0 is the phonon channel\n",
    "                    idx2s=[0, None],\n",
    "                    xlabel='Pulse Height (V)',\n",
    "                    ylabel='PCA Reconstruction Error',\n",
    "                    marker='.',\n",
    "                    xran=xr,\n",
    "                    yran=yr,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For determining the cut efficiency later on, we simulate now a dataset of pulses continuously distributed throughout the pulse height range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.simulate_pulses(path_sim=PATH_PROC_DATA + FNAME_EFFICIENCY + '.h5',\n",
    "                      size_events=10000,  # should be below Nmbr of clean baselines, otherwise activate reuse_bl\n",
    "                      reuse_bl=True,\n",
    "                      ev_ph_intervals=[[0, 1.6], [0, 0.3]],\n",
    "                      t0_interval=[-15, 15],  # in ms\n",
    "                      rms_thresholds=[100, 100],\n",
    "                      fake_noise=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have all pulse height estimation methods at hand, we calculate our usual estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_eff = ai.DataHandler(run=RUN,\n",
    "                    module=MODULE,\n",
    "                    channels=RDT_CHANNELS)\n",
    "\n",
    "dh_eff.set_filepath(path_h5=PATH_PROC_DATA,\n",
    "                fname=FNAME_EFFICIENCY,\n",
    "                appendix=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_eff.apply_of()\n",
    "dh_eff.calc_mp(type='events')\n",
    "dh_eff.apply_sev_fit(type='events', down=DOWN_SEF, verb=True, t0_bounds=(-25, 25), processes=PROCESSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we want to use anomaly detection methods later on, we build a data set of clean pulses. For these, we simulate baselines instead of using measured ones, to save the measured baselines for the efficiency calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_hw.simulate_pulses(path_sim=PATH_PROC_DATA + FNAME_TRAINING + '.h5',\n",
    "                      size_events=5000,  # should be below Nmbr of clean baselines, otherwise activate reuse_bl\n",
    "                      reuse_bl=True,\n",
    "                      ev_ph_intervals=[[0, 1.6], [0, 0.3]],\n",
    "                      t0_interval=[-20, 20],  # in ms\n",
    "                      rms_thresholds=[4e-6, 8e-6],\n",
    "                      fake_noise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have our usual features ready, we calculate main parameters, fits and filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_train = ai.DataHandler(run=RUN,\n",
    "                    module=MODULE,\n",
    "                    channels=RDT_CHANNELS)\n",
    "\n",
    "dh_train.set_filepath(path_h5=PATH_PROC_DATA,\n",
    "                fname=FNAME_TRAINING,\n",
    "                appendix=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_train.apply_of()\n",
    "dh_train.calc_mp(type='events')\n",
    "dh_train.calc_additional_mp(type='events')\n",
    "dh_train.apply_sev_fit(type='events', down=DOWN_SEF, name_appendix='_down{}'.format(DOWN_SEF), \n",
    "                       verb=True, t0_bounds=(-25, 25), processes=PROCESSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all we gonna do on the hardware data. Proceed with the trigger script, afterwards do quality cuts and energy calibration in the stream notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
